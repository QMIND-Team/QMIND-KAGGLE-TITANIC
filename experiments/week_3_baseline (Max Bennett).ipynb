{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import keras\n",
    "#import seaborn as sns\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\", category = DeprecationWarning)\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline at a glance:\n",
    "load data -> cleaning/preprocessing -> feature eng -> format data for modelling -> fit model -> evaluate model -> generate submission file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../input/train.csv')\n",
    "df_test = pd.read_csv('../input/test.csv')\n",
    "\n",
    "# for convenience\n",
    "datasets = [df_train, df_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Basic cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing values\n",
    "mAge = pd.concat((df_train['Age'], df_test['Age']), axis=0).mean()\n",
    "medFare = pd.concat((df_train['Fare'], df_test['Fare']), axis=0).median()\n",
    "for df in datasets:\n",
    "    df['Age'] = df['Age'].fillna(mAge)\n",
    "    df['Fare'] = df['Fare'].fillna(medFare)\n",
    "    df['Embarked'] = df['Embarked'].fillna('S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find fare bins based on frequency rather than value\n",
    "garbage, fare_bins = pd.qcut(df_train['Fare'].append(df_test['Fare']), 4, retbins=True)\n",
    "# loops through both train and test set for convenience\n",
    "for df in datasets:\n",
    "    # bin age\n",
    "    df['Age_binned'] = pd.cut(df['Age'], [0,16,32,48,64,200], labels = [0,1,2,3,4], retbins=False)\n",
    "    \n",
    "    # bin fare\n",
    "    df['Fare_binned'] = pd.cut(df['Fare'], fare_bins, labels = [0,1,2,3], include_lowest=True, retbins=False)\n",
    "    \n",
    "    # family features\n",
    "    df['Family_size'] = df_train['SibSp'] + df_train['Parch']\n",
    "    df['Is_Alone'] = (df['Family_size'] == 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unneeded rows\n",
    "for df in datasets:\n",
    "    df.drop(['Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Name', 'Fare_binned'], axis=1, inplace=True)\n",
    "    \n",
    "# need to keep passengerID for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Format data for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode categoricals \n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df_train['Sex'])\n",
    "df_train.loc[:,'Sex'] = le.transform(df_train['Sex'])\n",
    "df_test.loc[:,'Sex'] = le.transform(df_test['Sex'])\n",
    "\n",
    "le.fit(df_train['Embarked'])\n",
    "df_train['Embarked'] = le.transform(df_train['Embarked'])\n",
    "df_test['Embarked'] = le.transform(df_test['Embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into X and y, and select features to use\n",
    "X_train = df_train.drop(['Survived', 'PassengerId'], axis=1)\n",
    "y_train = df_train['Survived']\n",
    "X_test = df_test.drop(['PassengerId'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxbennett/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Split data X and y into train and val sets\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.2)\n",
    "\n",
    "    \n",
    "#X = X_train.values\n",
    "#y = y_train.values\n",
    "\n",
    "#X_test_ = X_test.values\n",
    "#X_test_ = X_test_.astype(np.float64, copy =0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId    0\n",
       "Survived       0\n",
       "Pclass         0\n",
       "Sex            0\n",
       "Embarked       0\n",
       "Age_binned     0\n",
       "Family_size    0\n",
       "Is_Alone       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_53 (Dense)             (None, 9)                 63        \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 9)                 90        \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 5)                 50        \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 209\n",
      "Trainable params: 209\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu', input_dim = 6))\n",
    "model.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(units = 5, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train (or fit) the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-259-929d3b5f7da8>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-259-929d3b5f7da8>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    callbacks = [EarlyStopping(monitor'val_loss', patience = 2),\u001b[0m\n\u001b[0m                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "#callbacks = [EarlyStopping(monitor'val_loss', patience = 2),\n",
    "            #ModelCheckpoint(filepath = 'best_model.h5', monitor = 'val_loss', save_best_only = Ture)]\n",
    "\n",
    "\n",
    "#model.compile(\n",
    "# optimizer = \"adam\",\n",
    "# loss = \"binary_crossentropy\",\n",
    "# metrics = [\"accuracy\"]\n",
    "#)\n",
    "\n",
    "model.fit(X, y, batch_size = 32, epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891/891 [==============================] - 0s 255us/step\n",
      "acc: 61.62%\n"
     ]
    }
   ],
   "source": [
    "# define a scoring function\n",
    "#def acc(y: np.array, y_pred: np.array) -> float:\n",
    "    #return np.sum(y_pred==y)/len(y)\n",
    "    \n",
    "results = model.fit(X_train, y_train, epochs = 100, verbose = 0, validation_data = (X_test, y_test))\n",
    "y_val = np.round(model.predict(X_val))\n",
    "\n",
    "#y_pred_test = pd.DataFrame(predictions)\n",
    "    \n",
    "    \n",
    "#pred = model.predict(X_val)\n",
    "#y_\n",
    "\n",
    "#pred = np.argmax(pred,axis=1)\n",
    "#y_compare = np.argmax(y_val,axis=1)\n",
    "#score = metrics.accuracy(y_compare, pred)\n",
    "\n",
    "scores = model.evaluate(X,y,batch_size = 30)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Test-Accuracy:\", np.mean(results.history[\"val_acc\"]))\n",
    "\n",
    "#history = model.fit(X,y, epochs = 200, batch_size = 10, verbose = 0)\n",
    "\n",
    "#print(history.history.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print('LR train set accuracy', acc(y_train, y_pred_trn_lr))\n",
    "#print('LR val set accuracy', acc(y_val, y_pred_val_lr))\n",
    "#print('RF train set accuracy', acc(y_train, y_pred_trn_rf))\n",
    "#print('RF val set accuracy', acc(y_val, y_pred_val_rf))\n",
    "\n",
    "#plt.plot(history.history['acc'])\n",
    "#plt.plot(history.history['val_acc'])\n",
    "#plt.title('Model Accuracy')\n",
    "#plt.ylabel('Accuracy')\n",
    "#plt.xlabel('Epoch')\n",
    "#plt.legend(['train', 'test'], loc='upper left')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit on whole dataset\n",
    "#lr.fit(X, y)\n",
    "\n",
    "# Predict for test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "y_pred_test = (y_pred > 0.5).astype(int).reshape(X_test_.shape[0])\n",
    "\n",
    "# Create a Kaggle submission\n",
    "sub = pd.DataFrame({'PassengerId': df_test['PassengerId'],\n",
    "                    'Survived': y_pred_test})\n",
    "\n",
    "\n",
    "sub.to_csv('week_3_baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
