### Experiment 1: Familysurvived feature (Kaelan)
* This is a relatively advanced new feature. If someone wants to take this on, I've provided code from another kernel that does it. Your job will be to implement that code into our own pipeline.
* https://www.kaggle.com/konstantinmasich/titanic-0-82-0-83, see "adding family survival" part

### Experiment 2: Putting basic models through a voting classifier (Tania)
* Sklearn has a voting classifier which essentially takes your basic models and combines them. This is a great way to ensemble the relatively strong performances of individual models.
* However, before doing this, make sure you make submissions with each individual basic model. Then, using LB score, give more weight on the voting classifier to the models that do well.
* If you haven't, make sure to use standard scaler on every feature before fitting

### Experiment 3: Improvements to LightGBM (Mitch)
* Finish up basic model
* Get deeper understanding of theory and meaning of main hyperparameters

### Experiment 4: Improve NN (Max)
* Play around and add things (Max already has a bunch of ideas)